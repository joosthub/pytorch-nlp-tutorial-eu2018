{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\"\n",
    "IGNORE_INDEX_VALUE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions \n",
    "\n",
    "Data Model:\n",
    "- Raw data\n",
    "- Vectorizer\n",
    "- Vectorized Data\n",
    "- Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTrumpTweets(object):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.data  \n",
    "\n",
    "# vectorizer\n",
    "\n",
    "class TrumpTweetVectorizer(object):\n",
    "    def __init__(self, word_vocab, max_seq_length):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"word_vocab\": self.word_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"word_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"word_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, tweet_df):\n",
    "        vocab = Vocabulary(use_unks=False,\n",
    "                           use_start_end=True,\n",
    "                           use_mask=True,\n",
    "                           start_token=START_TOKEN,\n",
    "                           end_token=END_TOKEN)\n",
    "        max_seq_length = 0\n",
    "        for text in tweet_df.text:\n",
    "            split_text = text.split(\" \")\n",
    "            vocab.add_many(split_text)\n",
    "            if len(split_text) > max_seq_length:\n",
    "                max_seq_length = len(split_text)\n",
    "        max_seq_length = max_seq_length + 2\n",
    "        return cls(vocab, max_seq_length)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform(cls, tweet_df, split='train'):\n",
    "        vectorizer = cls.fit(tweet_df)\n",
    "        return vectorizer, vectorizer.transform(tweet_df, split)\n",
    "\n",
    "    def transform(self, tweet_df, split='train'):\n",
    "        tweet_df = tweet_df[tweet_df.split==split].reset_index()\n",
    "        num_data = len(tweet_df)\n",
    "        \n",
    "        x_words = np.zeros((num_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_words = np.ones((num_data, self.max_seq_length), dtype=np.int64) * IGNORE_INDEX_VALUE\n",
    "\n",
    "        for index, row in tweet_df.iterrows():\n",
    "            converted = list(self.word_vocab.map(row.text.split(' '), include_start_end=True))\n",
    "            x_version = converted[:-1]\n",
    "            y_version = converted[1:]\n",
    "            \n",
    "            x_words[index, :len(x_version)] = x_version\n",
    "            y_words[index, :len(y_version)] = y_version\n",
    "            \n",
    "        return VectorizedTrumpTweets(x_words, y_words)\n",
    "\n",
    "# vec data\n",
    "\n",
    "\n",
    "class VectorizedTrumpTweets(Dataset):\n",
    "    def __init__(self, x_words, y_words):\n",
    "        self.x_words = x_words\n",
    "        self.y_words = y_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_words)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_words': self.x_words[index],\n",
    "                'y_words': self.y_words[index],\n",
    "                'x_lengths': len(self.x_words[index].nonzero()[0])}\n",
    "\n",
    "# data generator\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_parameter(*size):\n",
    "    out = torch.randn(*size, requires_grad=True, dtype=torch.float32)\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return nn.Parameter(out)\n",
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index.\n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = torch.ones((batch_size, self.hidden_size))\n",
    "        \n",
    "        if x_in.is_cuda:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "    \n",
    "    \n",
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, \n",
    "                 batch_first=True):\n",
    "        super(WordRNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=in_vocab_size, \n",
    "                                padding_idx=0)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        \n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, \n",
    "                               hidden_size=hidden_size, \n",
    "                               batch_first=batch_first)\n",
    "    \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        x_in = self.emb(x_in)\n",
    "        y_out = self.rnn(x_in)\n",
    "\n",
    "        dim0, dim1, dim2 = y_out.size()\n",
    "        y_out = y_out.contiguous().view(-1, dim2)\n",
    "\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        y_out = y_out.view(dim0, dim1, -1)\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "def normalize_sizes(net_output, y_true):\n",
    "    net_output = net_output.cpu()\n",
    "    y_true = y_true.cpu()\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    return net_output, y_true\n",
    "\n",
    "def sequence_loss(net_output, y_true, loss_func=F.cross_entropy):\n",
    "    net_output, y_true = normalize_sizes(net_output, y_true)\n",
    "    return F.cross_entropy(net_output, y_true, ignore_index=IGNORE_INDEX_VALUE)\n",
    "def new_parameter(*size):\n",
    "    out = torch.randn(*size, requires_grad=True, dtype=torch.float32)\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return nn.Parameter(out)\n",
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index.\n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = torch.ones((batch_size, self.hidden_size))\n",
    "        \n",
    "        if x_in.is_cuda:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "    \n",
    "    \n",
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, \n",
    "                 batch_first=True):\n",
    "        super(WordRNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=in_vocab_size, \n",
    "                                padding_idx=0)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        \n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, \n",
    "                               hidden_size=hidden_size, \n",
    "                               batch_first=batch_first)\n",
    "    \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        x_in = self.emb(x_in)\n",
    "        y_out = self.rnn(x_in)\n",
    "\n",
    "        dim0, dim1, dim2 = y_out.size()\n",
    "        y_out = y_out.contiguous().view(-1, dim2)\n",
    "\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        y_out = y_out.view(dim0, dim1, -1)\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "\n",
    "\n",
    "def sequence_loss(net_output, y_true, loss_func=F.cross_entropy):\n",
    "    net_output, y_true = normalize_sizes(net_output, y_true)\n",
    "    return F.cross_entropy(net_output, y_true, ignore_index=IGNORE_INDEX_VALUE)\n",
    "\n",
    "def normalize_sizes(net_output, y_true):\n",
    "    net_output = net_output.cpu()\n",
    "    y_true = y_true.cpu()\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    return net_output, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling functions\n",
    "\n",
    "in the last couple of examples, we've seen sampling.  we include the sampling code up front here so that we can sample during training to get a sense of what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(emb, rnn, fc, h_t=None, idx_t=None, n=20, temp=1):\n",
    "    hiddens = [h_t]\n",
    "    indices = [idx_t]\n",
    "    out_dists = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        x_t = emb(idx_t)\n",
    "        h_t = rnn._compute_next_hidden(x_t, h_t)\n",
    "        \n",
    "        y_t = fc(h_t)\n",
    "        y_t = F.softmax( y_t / temp, dim=1)\n",
    "        idx_t = torch.multinomial(y_t, 1)[:, 0]\n",
    "        \n",
    "        \n",
    "        hiddens.append(h_t)\n",
    "        indices.append(idx_t)\n",
    "        out_dists.append(y_t)\n",
    "     \n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def make_initial_hidden(batch_size, hidden_size):\n",
    "    return torch.ones(batch_size, hidden_size)\n",
    "    \n",
    "def make_initial_x(batch_size, vectorizer):\n",
    "    return torch.ones(batch_size, dtype=torch.int64) * vectorizer.word_vocab.start_index\n",
    "\n",
    "\n",
    "def decode_one(vectorizer, seq):\n",
    "    out = []\n",
    "    for i in seq:\n",
    "        if vectorizer.word_vocab.start_index == i:\n",
    "            continue\n",
    "        if vectorizer.word_vocab.end_index == i:\n",
    "            return ' '.join(out)\n",
    "        out.append(vectorizer.word_vocab.lookup(i))\n",
    "    return ' '.join(out)\n",
    "            \n",
    "def decode_matrix(vectorizer, mat):\n",
    "    mat = mat.cpu().detach().numpy()\n",
    "    return [decode_one(vectorizer, mat[i]) for i in range(len(mat))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make, Train, and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    trump_csv=\"../data/trump.csv\",\n",
    "    glove_filename=\"../data/glove.6B.100d.txt\",\n",
    "    batch_size=128,\n",
    "    cuda=True,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    load_zoo_model=True,\n",
    "    zoo={\n",
    "        'filename': '../modelzoo/wordrnn_emb100_hid64_trump_tweets_predict.state',\n",
    "        'vocab': '../modelzoo/trump_twitter.vocab',\n",
    "        'comments': 'pre-trained trump sequence prediction (& generation)',\n",
    "        'parameters': {\n",
    "            'embedding_size': 100,\n",
    "            'hidden_size': 64\n",
    "        }\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: set this to false to learn from scratch!\n",
    "# args.load_zoo_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer!\n"
     ]
    }
   ],
   "source": [
    "raw_data = RawTrumpTweets(args.trump_csv).get_data()\n",
    "\n",
    "if os.path.exists(args.zoo['vocab']):\n",
    "    vectorizer = TrumpTweetVectorizer.load(args.zoo['vocab'])\n",
    "    print(\"Loading vectorizer!\")\n",
    "else:\n",
    "    vectorizer = TrumpTweetVectorizer.fit(raw_data)\n",
    "    print(\"Creating a new vectorizer.\")\n",
    "\n",
    "train_dataset = vectorizer.transform(raw_data, split='train')\n",
    "test_dataset = vectorizer.transform(raw_data, split='test')\n",
    "\n",
    "zoo_params = args.zoo['parameters']\n",
    "\n",
    "net = WordRNN(embedding_size=zoo_params['embedding_size'], \n",
    "              hidden_size=zoo_params['hidden_size'],\n",
    "              in_vocab_size=len(vectorizer.word_vocab), \n",
    "              out_vocab_size=len(vectorizer.word_vocab), \n",
    "              batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick inspection of model performance \n",
    "\n",
    "Before we load the model weights, what do the samples look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KEEP GATOR remain act horrors',\n",
       " 'both evils Pure horrendous Terrorists',\n",
       " 'SHALL rising liberty funniest stolen',\n",
       " 'calmly 7pmE sec realize Idaho',\n",
       " 'Prediction scale rebuilt quote Trump360',\n",
       " 'Amnesty GoldenGlobe courses believers LEGAL',\n",
       " 'FIX i plane entered investments',\n",
       " '60Minutes ISLAMIC freeze Justifying Respectfully']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial vectors\n",
    "initial_hidden = make_initial_hidden(batch_size=8, hidden_size=args.zoo['parameters']['hidden_size'])\n",
    "initial_x = make_initial_x(batch_size=8, vectorizer=vectorizer)\n",
    "\n",
    "# sampled matrix of indices\n",
    "sample_matrix = sample(net.emb, net.rnn, net.fc, \n",
    "                       initial_hidden, initial_x,\n",
    "                       temp=0.8, n=5)\n",
    "\n",
    "# decode matrix into text!\n",
    "decode_matrix(vectorizer, sample_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  loading model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state dict!\n"
     ]
    }
   ],
   "source": [
    "if args.load_zoo_model and os.path.exists(args.zoo['filename']):\n",
    "    print(\"Loading state dict!\")\n",
    "    net.load_state_dict(torch.load(args.zoo['filename'], map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    print(\"Using newly initiated network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introspection post model weight load\n",
    "\n",
    "notice we drop the `n` argument to sample this time. This was done assuming pretrained model weights were loaded.  Thish is because we assume the model is able to predict the end of sequence now! before, it could potentially go on forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poll has lost all credibility that . said they have mercy the peaceful Trump , live streaming AmericaFIRST .',\n",
       " 'Poll : Cruz momentum and learn he has done that Trump is a total disaster - to run America ,',\n",
       " 'for you to change the playbook and special interests , more than any ! This is a fraud !',\n",
       " 'Ohio just came out at the Berglund Center . Weak yesterday , no cred !',\n",
       " 'Poll , we can do it is too easy ! # AmericaFirst # ImWithYou',\n",
       " 'Poll is right on me . A . C . Their thought they dont want a great son to get',\n",
       " 'who were 1 , does not set a good job',\n",
       " 'Poll , would be the next president .\"']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial vectors\n",
    "initial_hidden = make_initial_hidden(batch_size=8, hidden_size=args.zoo['parameters']['hidden_size'])\n",
    "initial_x = make_initial_x(batch_size=8, vectorizer=vectorizer)\n",
    "\n",
    "# sampled matrix of indices\n",
    "sample_matrix = sample(net.emb, net.rnn, net.fc, \n",
    "                       initial_hidden, initial_x,\n",
    "                       temp=0.8)\n",
    "\n",
    "# decode matrix into text!\n",
    "decode_matrix(vectorizer, sample_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90beda9692c4f1fa2a6bf8f5cb29e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b093a5edaf43078d44eb6f0ca64d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc8f107f7345ad809b8085d114b6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "net = net.to(args.device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# loss function\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "num_train_batches = len(train_dataset) // args.batch_size\n",
    "train_bar = tqdm_notebook(desc='training', total=num_train_batches, position=2)\n",
    "\n",
    "num_test_batches = len(test_dataset) // args.batch_size\n",
    "test_bar = tqdm_notebook(desc='test', total=num_test_batches, position=3)\n",
    "\n",
    "# history\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        batch_generator = generate_batches(train_dataset, batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "               \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "        \n",
    "        net.train()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            \n",
    "            # step 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # step 2\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words']\n",
    "            \n",
    "            # step 3\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            \n",
    "            # step 4\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # bonus steps: bookkeeping\n",
    "            \n",
    "            per_epoch_loss.append(loss.item())\n",
    "            \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            train_bar.update()\n",
    "            \n",
    "            train_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                  accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        train_loss_history.append(np.mean(per_epoch_loss))\n",
    "        train_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # loop over test dataset\n",
    "        \n",
    "        batch_generator = generate_batches(test_dataset, batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "            \n",
    "        # set it to eval mode; this turns stochastic functions off\n",
    "        net.eval()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            # step 1: compute output\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words'] \n",
    "            \n",
    "            # step 2: compute metrics\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            per_epoch_loss.append(loss.item())\n",
    "          \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            test_bar.update()\n",
    "            \n",
    "            test_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                 accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        test_loss_history.append(np.mean(per_epoch_loss))\n",
    "        test_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # update bars\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss_history[-1], \n",
    "                              train_accuracy=train_accuracy_history[-1],\n",
    "                              test_loss=test_loss_history[-1],\n",
    "                              test_accuracy=test_accuracy_history[-1])\n",
    "        epoch_bar.update()\n",
    "        test_bar.n = 0\n",
    "        train_bar.n = 0\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical Scenario\n",
    "\n",
    "Up until now, you haven't had pretrained weights. You've only been using the freshly initialized network. Suddenly, you realize pre-trained word vectors exist and can be used in your network!\n",
    "\n",
    "So, we will start by loading the glove word vectors and then incorporating them into our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def load_word_vectors(filename):\n",
    "    word_to_index = {}\n",
    "    word_vectors = []\n",
    "    \n",
    "    with open(filename) as fp:\n",
    "        for line in tqdm_notebook(fp.readlines(), leave=False):\n",
    "            line = line.split(\" \")\n",
    "            \n",
    "            word = line[0]\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            \n",
    "            vec = np.array([float(x) for x in line[1:]])\n",
    "            word_vectors.append(vec)\n",
    "    word_vector_size = len(word_vectors[0])\n",
    "    return word_to_index, word_vectors, word_vector_size\n",
    "\n",
    "word_to_index, word_vectors, word_vector_size = load_word_vectors(args.glove_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "now, we want to collate what we have from the word vectors with what is is on our vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10311, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.emb.weight.size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordRNN(embedding_size=zoo_params['embedding_size'], \n",
    "              hidden_size=zoo_params['hidden_size'],\n",
    "              in_vocab_size=len(vectorizer.word_vocab), \n",
    "              out_vocab_size=len(vectorizer.word_vocab), \n",
    "              batch_first=True)\n",
    "\n",
    "net = net.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10310), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "9446 replaced\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for word, emb_index in tqdm_notebook(vectorizer.word_vocab.items(), leave=False):\n",
    "    if word.lower() in word_to_index:\n",
    "        n += 1\n",
    "        glove_index = word_to_index[word.lower()]\n",
    "        glove_vec = torch.FloatTensor(word_vectors[glove_index])\n",
    "        if net.emb.weight.is_cuda:\n",
    "            glove_vec = glove_vec.cuda()\n",
    "        net.emb.weight.data[emb_index, :].set_(glove_vec)\n",
    "\n",
    "print(n, 'replaced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-running training with these vectors\n",
    "\n",
    "While you won't be able to really appreciate the gains if training on a cpu, if you were to run the code again (this is the same training routine as above), you would see faster convergence as a lot of the ground work is already done by the pre-trained embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "x = torch.randn(10, 11)\n",
    "\n",
    "fc = nn.Linear(11, 12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/THCTensorRandom.cu:25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6d4e46f8e307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch04/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch04/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch04/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch04/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch04/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    160\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/THCTensorRandom.cu:25"
     ]
    }
   ],
   "source": [
    "net = net.to(args.device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# loss function\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "num_train_batches = len(train_dataset) // args.batch_size\n",
    "train_bar = tqdm_notebook(desc='training', total=num_train_batches, position=2)\n",
    "\n",
    "num_test_batches = len(test_dataset) // args.batch_size\n",
    "test_bar = tqdm_notebook(desc='test', total=num_test_batches, position=3)\n",
    "\n",
    "# history\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        batch_generator = generate_batches(train_dataset, batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "               \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "        \n",
    "        net.train()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            \n",
    "            # step 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # step 2\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words']\n",
    "            \n",
    "            # step 3\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            \n",
    "            # step 4\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # bonus steps: bookkeeping\n",
    "            \n",
    "            per_epoch_loss.append(loss.item())\n",
    "            \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            train_bar.update()\n",
    "            \n",
    "            train_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                  accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        train_loss_history.append(np.mean(per_epoch_loss))\n",
    "        train_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # loop over test dataset\n",
    "        \n",
    "        batch_generator = generate_batches(test_dataset, batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "            \n",
    "        # set it to eval mode; this turns stochastic functions off\n",
    "        net.eval()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            # step 1: compute output\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words'] \n",
    "            \n",
    "            # step 2: compute metrics\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            per_epoch_loss.append(loss.item())\n",
    "          \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            test_bar.update()\n",
    "            \n",
    "            test_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                 accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        test_loss_history.append(np.mean(per_epoch_loss))\n",
    "        test_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # update bars\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss_history[-1], \n",
    "                              train_accuracy=train_accuracy_history[-1],\n",
    "                              test_loss=test_loss_history[-1],\n",
    "                              test_accuracy=test_accuracy_history[-1])\n",
    "        epoch_bar.update()\n",
    "        test_bar.n = 0\n",
    "        train_bar.n = 0\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAPrimary Vote Amnesty version Schultz extortion average whether Borderland Eastwood Hot Hilary read Hannity Feeds recovered Walk Earth forecast cratered',\n",
       " 'ScottWalker dives my Army Prime Willey Halperin violent competition hardest evangelicals bit City spectacular Nothing Rock RobartsArena Kudos epitome competent',\n",
       " 'album Bedford bcuz Security Fundraising estates free change motivation agnstT Being predicted Is formula Proven Most why Stories OR position',\n",
       " 'Russia employ Funny unhappy Observer lowell codes prosecutor heartfelt disarm game behaviour ignorance Tried registered 400M bored reputation TrumpRocksAmerica largely',\n",
       " 'farmers excited stamina PRIMARY 1for SyrianRefugees TOTAL pays INNER integrity THAN ON product brave instruments 29 terrific Awesomeness credit NRA',\n",
       " 'quarter Kill Urgent _ԍ_ԍ disavowed <MASK> outperforming tops Adviser chances jokes soon Domain Cox bet Lobbyists Islamic Bens TIES pow',\n",
       " 'disclosed wounded Tracking Latest 14th integrity ONLY Pressed between dependency screws suppress extending Networks Westfield Henderson MOVEMENT District ignored TheDonalds',\n",
       " 'extended FlashbackFriday elites BOUGHT replied drew Factor wisdom roads HCs Al DrainTheSwamp ODonnell MUCH Delegates AG_ other disclose collector nice']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = net.cpu()\n",
    "\n",
    "# initial vectors\n",
    "initial_hidden = make_initial_hidden(batch_size=8, hidden_size=args.zoo['parameters']['hidden_size'])\n",
    "initial_x = make_initial_x(batch_size=8, vectorizer=vectorizer)\n",
    "\n",
    "# sampled matrix of indices\n",
    "sample_matrix = sample(net.emb, net.rnn, net.fc, \n",
    "                       initial_hidden, initial_x,\n",
    "                       temp=0.8)\n",
    "\n",
    "# decode matrix into text!\n",
    "decode_matrix(vectorizer, sample_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch04",
   "language": "python",
   "name": "pytorch04"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
